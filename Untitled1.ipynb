{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Error messages \n",
    "INVALID_FILETYPE_MSG = \"Error: Invalid file format. %s must be a .txt file.\"\n",
    "INVALID_PATH_MSG = \"Error: Invalid file path/name. Path %s does not exist.\"\n",
    "\n",
    "\n",
    "def read_js_list():\n",
    "    global data\n",
    "    with open('lp.txt','r') as fp:\n",
    "        dataa=fp.read()\n",
    "    dataa=str(dataa)\n",
    "    dataa=dataa.replace('\\n',' ')\n",
    "    dataa=dataa.replace('\\t',' ')\n",
    "    data=(dataa.split(' '))\n",
    "    return data\n",
    "\n",
    "#HTML parser\n",
    "def html_parser(url,soup):\n",
    "\n",
    "    soup_str=str(soup)                                                 #Converting the BeautifulSoup object into string \n",
    "    \n",
    "    length_of_doc=len(soup_str)                                        #Finding the total length of the document\n",
    "    \n",
    "    iframes=soup.find_all('iframe')                                    #Finding the number of iframes\n",
    "    no_of_iframes=len(iframes)\n",
    "    \n",
    "    iframe_src=[url+i[\"src\"] for i in soup.find_all(\"iframe\",src=True)] #Number of iframes links\n",
    "    no_of_iframelinks=len(iframe_src)\n",
    "\n",
    "    js_src=[url+i[\"src\"] for i in soup.find_all(\"script\",src=True)]     #Number of Javascript links\n",
    "    no_of_jslinks=len(js_src)\n",
    "\n",
    "    hyperlinks=soup.find_all('a')                                       #Finding the number of hyperlinks\n",
    "    no_of_hyperlinks=len(hyperlinks)\n",
    "    \n",
    "    tags=[]                                                             #Finding the types and number of tags \n",
    "    for tag in soup.find_all(True):\n",
    "        tags.append(tag.name)\n",
    "    uniq,counts=np.unique(tags,return_counts=True)\n",
    "    tag_n_count=dict(zip(uniq,counts))\n",
    "\n",
    "    tag_types=len(uniq)\n",
    "    \n",
    "    j=0                                                            #Finding the number of symmetrical and assymmetrical HTML tags\n",
    "    utag_count=0\n",
    "    stag_count=0\n",
    "    for i in tag_n_count:\n",
    "        if(tag_n_count[i]%2!=0):\n",
    "            utag_count=(utag_count)+(tag_n_count[i])\n",
    "        else:\n",
    "            stag_count=stag_count+(tag_n_count[i])\n",
    "            \n",
    "    hidden_objs=soup_str.count('\"hidden\"')                              #Finding the number of hidden objects occurences\n",
    "    \n",
    "    concat=soup_str.count('concat')                                     #Finding the number of string concatenation functions\n",
    "    \n",
    "    html['Length_of_document']=html['Length_of_document']+length_of_doc\n",
    "    print('\\nLength of the document is : ',length_of_doc)\n",
    "\n",
    "    html['No_of_iframes']=html['No_of_iframes']+no_of_iframes\n",
    "    print('\\nNumber of iframes : ',no_of_iframes)\n",
    "\n",
    "    html['No_of_iframelinks']=html['No_of_iframelinks']+no_of_iframelinks\n",
    "    print('\\nNo. of iframes links : ',no_of_iframelinks)\n",
    "\n",
    "    html['No_of_jslinks']=html['No_of_jslinks']+no_of_jslinks\n",
    "    print('\\nNo. of javascript links : ',no_of_jslinks)\n",
    "\n",
    "    html['No_of_hyperlinks']=html['No_of_hyperlinks']+no_of_hyperlinks\n",
    "    print('\\nNumber of hyperlinks is : ',no_of_hyperlinks)\n",
    "\n",
    "    html['Types_of_tags']=html['Types_of_tags']+tag_types\n",
    "    print('\\nThe types of tags is :',tag_types)\n",
    "    \n",
    "    html['Unsymmetrical_tags']=html['Unsymmetrical_tags']+utag_count\n",
    "    html['Symmetrical_tags']=html['Symmetrical_tags']+stag_count\n",
    "    print('\\nThe number of assymmetrical HTML tags to symmetrical tags is : ',utag_count,':',stag_count)\n",
    "\n",
    "    html['Hidden_objects']=html['Hidden_objects']+hidden_objs\n",
    "    print('\\nThe number of hidden objects occurences is : ',hidden_objs)\n",
    "\n",
    "    html['No_of_concatenation']=html['No_of_concatenation']+concat\n",
    "    print('Concatenation : ',concat)\n",
    "    \n",
    "    ###Extracting the client side javascript features\n",
    "    \n",
    "    data=read_js_list()                     #Reading in the js keywords into a list\n",
    "\n",
    "    a=0\n",
    "    for x in data:                          #Counting the number of javascript keywords\n",
    "        a=a+soup_str.count(str(x))\n",
    "\n",
    "\n",
    "    escape=soup_str.count('escape')\n",
    "    unescape=soup_str.count('unescape')  \n",
    "    link=soup.find_all('link')\n",
    "    linko=len(link)\n",
    "    execo=soup_str.count('execo')\n",
    "    search=soup_str.count('search')\n",
    "    evalo=soup_str.count('eval')\n",
    "    decode=soup_str.count('decodeURI')\n",
    "    encode=soup_str.count('encodeURI')\n",
    "    concat=soup_str.count('concat')\n",
    "    \n",
    "    js['Keywords']=js['Keywords']+a\n",
    "    print('No. of keywords is : ',a)\n",
    "\n",
    "    js['escape']=js['escape']+escape\n",
    "    print('Escape : ',escape)\n",
    "\n",
    "    js['unescape']=js['unescape']+unescape\n",
    "    print('Unescape : ',unescape)\n",
    "\n",
    "    js['link']=js['link']+linko\n",
    "    print('Link : ',linko)\n",
    "\n",
    "    js['execo']=js['execo']+execo\n",
    "    print('Exec : ',execo)\n",
    "\n",
    "    js['search']=js['search']+search\n",
    "    print('Search : ',search)\n",
    "\n",
    "    js['eval']=js['eval']+evalo\n",
    "    print('Eval : ',evalo)\n",
    "\n",
    "    js['decode']=js['decode']+decode\n",
    "    print('decodeURI : ',decode)\n",
    "\n",
    "    js['encode']=js['encode']+encode\n",
    "    print('encodeURI : ',encode)\n",
    "\n",
    "    js['concat']=js['concat']+concat\n",
    "    print('Concatenation : ',concat)\n",
    "            \n",
    "    return 0\n",
    "\n",
    "#Javascript parser for associated files\n",
    "def jsparser_links(url,r):\n",
    "    \n",
    "    soup=opening_url(url,r)\n",
    "    \n",
    "    if(soup==0):\n",
    "        return 0 \n",
    "    \n",
    "    soup_str=str(soup)                                        #Converting the BeautifulSoup object into string \n",
    "    \n",
    "    data=read_js_list()                                                #Reading in the js keywords into a list\n",
    "    \n",
    "    length_of_jsdoc=len(soup_str)                                     #Finding the total length of the document\n",
    "    \n",
    "    iframes=soup.find_all('iframe')                                              #Finding the number of iframes\n",
    "    no_of_jsiframes=len(iframes)\n",
    "\n",
    "    a=0\n",
    "    for x in data:                                                             #Counting the number of keywords\n",
    "        a=a+soup_str.count(str(x))\n",
    "    \n",
    "    escape=soup_str.count('escape')\n",
    "    unescape=soup_str.count('unescape')  \n",
    "    link=soup.find_all('link')\n",
    "    linko=len(link)\n",
    "    execo=soup_str.count('execo')\n",
    "    search=soup_str.count('search')\n",
    "    evalo=soup_str.count('eval')\n",
    "    decode=soup_str.count('decodeURI')\n",
    "    encode=soup_str.count('encodeURI')\n",
    "    concat=soup_str.count('concat')\n",
    " \n",
    "    js['Length_of_document']=js['Length_of_document']+length_of_jsdoc\n",
    "    print('Length of the document : ',length_of_jsdoc)\n",
    "    \n",
    "    js['No_of_iframes']=js['No_of_iframes']+no_of_jsiframes\n",
    "    print('Number of iframes : ',no_of_jsiframes)\n",
    "    \n",
    "    js['Keywords']=js['Keywords']+a\n",
    "    print('No. of keywords is : ',a)\n",
    "    \n",
    "    js['escape']=js['escape']+escape\n",
    "    print('Escape : ',escape)\n",
    "\n",
    "    js['unescape']=js['unescape']+unescape\n",
    "    print('Unescape : ',unescape)\n",
    "    \n",
    "    js['link']=js['link']+linko\n",
    "    print('Link : ',linko)\n",
    "    \n",
    "    js['execo']=js['execo']+execo\n",
    "    print('Exec : ',execo)\n",
    "\n",
    "    js['search']=js['search']+search\n",
    "    print('Search : ',search)\n",
    "    \n",
    "    js['eval']=js['eval']+evalo\n",
    "    print('Eval : ',evalo)\n",
    "    \n",
    "    js['decode']=js['decode']+decode\n",
    "    print('decodeURI : ',decode)\n",
    "    \n",
    "    js['encode']=js['encode']+encode\n",
    "    print('encodeURI : ',encode)\n",
    "\n",
    "    js['concat']=js['concat']+concat\n",
    "    print('Concatenation : ',concat)\n",
    "   \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "#Function which calls the html and javascript parser\n",
    "def parsero(url,r):\n",
    "    \n",
    "    soup=opening_url(url,r)                                            #Page retrieval request through the function call\n",
    "    \n",
    "    if(soup==0):\n",
    "        return 0\n",
    "    html_parser(url,soup)                            #Function called for the parsing of the necessary HTML elements\n",
    "\n",
    "    iframe_src=[i[\"src\"] for i in soup.find_all(\"iframe\",src=True)]            #Finding the related iframe files\n",
    "    js_src=[i[\"src\"] for i in soup.find_all(\"script\",src=True)]            #Finding the related javascript files   \n",
    "\n",
    "    ###Related iframe scraping        \n",
    "    if(len(iframe_src)):\n",
    "        for irr in iframe_src:                                                     #Looping through the iframes serially\n",
    "            if(url.count(irr)<=1):\n",
    "                ur=url+str(irr)\n",
    "                parsero(ur,r)\n",
    "            \n",
    "    ###Related javascript scraping\n",
    "    print(len(js_src))\n",
    "    if(len(js_src)):\n",
    "        read_js_list()                                                #Reading in the js keywords into a list\n",
    "        for jrr in js_src:                                      #Looping through the javascript files serially\n",
    "            if(url.count(jrr)<=1):\n",
    "                ur=url+str(jrr)\n",
    "                jsparser_links(ur,r)                               #Function called for the related javascript files parsing\n",
    "\n",
    "    return 0\n",
    "\n",
    "def opening_url(url,r):\n",
    "\n",
    "    page=r.get(url)                                #Retrieving the page \n",
    "    \n",
    "    if(page.status_code==200):                            #Check for the correct response\n",
    "        print('Page ',url,' retrieve success')\n",
    "        soup=BeautifulSoup(page.content,'html.parser')    #Making of the BeautifulSoup object\n",
    "        \n",
    "        return soup\n",
    "    \n",
    "    else:\n",
    "        print('Page retrieve failure')\n",
    "        time.sleep(0.2)\n",
    "        return 0\n",
    "\n",
    "def writingfile(df_html,df_js):\n",
    "    #with open('htmlFinal.csv','a') as fh:\n",
    "      #  df_html.to_csv(fh,header=False)\n",
    "    #with open('jsFinal.csv','a') as fj:\n",
    "        #df_js.to_csv(fj,header=False)\n",
    "    filename=\"htmlFinal\"\n",
    "    filename=filename+\".csv\"\n",
    "    df_html.to_csv(filename,index=False,sep=',')\n",
    "    print('The file has been written with name')\n",
    "    filename=\"jsFinal\"\n",
    "    filename=filename+\".csv\"\n",
    "    df_js.to_csv(filename,index=False,sep=',')\n",
    "    print('The file has been written with name')\n",
    "    return \n",
    "\n",
    "def main():\n",
    "\n",
    "    r=requests.session()                                                     #Creating a session for all the page requests\n",
    "    \n",
    "    global html\n",
    "    global js\n",
    "    slno=1\n",
    "    html=dict.fromkeys(['URL','Length_of_document','No_of_iframes','No_of_iframelinks','No_of_jslinks',\n",
    "                        'No_of_hyperlinks','Types_of_tags','Unsymmetrical_tags','Symmetrical_tags','Hidden_objects',\n",
    "                        'No_of_concatenation','Category'],0)\n",
    "    js=dict.fromkeys(['URL','Length_of_document','No_of_iframes','Keywords','escape','unescape','link','execo',\n",
    "                      'search','eval','decode','encode','concat','Category'],0)\n",
    "    \n",
    "    #Defining the dataframes for the html and javascript features\n",
    "    df_html=pd.DataFrame(columns=['URL','Length_of_document','No_of_iframes','No_of_iframelinks','No_of_jslinks',\n",
    "                                  'No_of_hyperlinks','Types_of_tags','Unsymmetrical_tags','Symmetrical_tags','Hidden_objects',\n",
    "                                  'No_of_concatenation','Category'])\n",
    "    df_js=pd.DataFrame(columns=['URL','Length_of_document','No_of_iframes','Keywords','escape','unescape','link','execo',\n",
    "                                'search','eval','decode','encode','concat','Category'])\n",
    "\n",
    "                                            \n",
    "    urls=[line.rstrip('\\n') for line in open('urls.txt')]                         #Extracting the urls form the file\n",
    "    for ur in urls:\n",
    "        if(ur.count('.')==1):\n",
    "            ur='http://www.'+ur\n",
    "            ur=ur+'/'\n",
    "        elif(ur.count('.')>=2):\n",
    "            ur='http://'+ur\n",
    "            ur=ur+'/'\n",
    "        parsero(ur,r)\n",
    "        df_html=df_html.append({'URL':ur,'Length_of_document':html['Length_of_document'],'No_of_iframes':html['No_of_iframes'],\n",
    "                           'No_of_iframelinks':html['No_of_iframelinks'],'No_of_jslinks':html['No_of_jslinks'],\n",
    "                           'No_of_hyperlinks':html['No_of_hyperlinks'],'Types_of_tags':html['Types_of_tags'],\n",
    "                           'Unsymmetrical_tags':html['Unsymmetrical_tags'],'Symmetrical_tags':html['Symmetrical_tags'],\n",
    "                           'Hidden_objects':html['Hidden_objects'],'No_of_concatenation':html['No_of_concatenation'],\n",
    "                           'Category':0},ignore_index=True)\n",
    "        df_js=df_js.append({'URL':ur,'Length_of_document':js['Length_of_document'],'No_of_iframes':js['No_of_iframes'],\n",
    "                           'Keywords':js['Keywords'],'escape':js['escape'],'unescape':js['unescape'],'link':js['link'],\n",
    "                           'execo':js['execo'],'search':js['search'],'eval':js['eval'],'decode':js['decode'],\n",
    "                           'encode':js['encode'],'concat':js['concat'],'Category':0},ignore_index=True)\n",
    "\n",
    "        html=dict.fromkeys(['URL','Length_of_document','No_of_iframes','No_of_iframelinks','No_of_jslinks',\n",
    "                                'No_of_hyperlinks','Types_of_tags','Unsymmetrical_tags','Symmetrical_tags','Hidden_objects',\n",
    "                                'No_of_concatenation','Category'],0)\n",
    "        js=dict.fromkeys(['URL','Length_of_document','No_of_iframes','Keywords','escape','unescape','link','execo',\n",
    "                              'search','eval','decode','encode','concat','Category'],0)\n",
    "            \n",
    "        print('****************************** ',slno,'th WEBSITE SCRAPED  ******************************************')\n",
    "        slno=slno+1\n",
    "        time.sleep(0.2)\n",
    "    writingfile(df_html,df_js)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
